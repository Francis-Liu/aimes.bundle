(mypyenv.bundle)francis@ip-10-243-91-117:~/bundle$ python simple_usage.py
******    Step 01: create a BundleManager instance    ******
SUCCEED!
******    Step 02: check BundleManager's active resource list    ******
[u'stampede.tacc.xsede.org', u'blacklight.psc.xsede.org', u'gordon.sdsc.xsede.org', u'trestles.sdsc.xsede.org']
SUCCEED!
******    Step 03: show each resource cluster's configuration    ******
Cluster stampede.tacc.xsede.org:
{
    "development": {
        "max_walltime": 432000,
        "num_procs_limit": 256,
        "queue_name": "development",
        "queued_jobs_limit": 1
    },
    "large": {
        "max_walltime": 432000,
        "num_procs_limit": 16384,
        "queue_name": "large",
        "queued_jobs_limit": 50
    },
    "largemem": {
        "max_walltime": 432000,
        "num_procs_limit": 128,
        "queue_name": "largemem",
        "queued_jobs_limit": 4
    },
    "normal": {
        "max_walltime": 432000,
        "num_procs_limit": 4096,
        "queue_name": "normal",
        "queued_jobs_limit": 50
    },
    "serial": {
        "max_walltime": 432000,
        "num_procs_limit": 16,
        "queue_name": "serial",
        "queued_jobs_limit": 8
    }
}
Cluster blacklight.psc.xsede.org:
{
    "batch": {
        "queue_name": "batch"
    },
    "batch_l2": {
        "max_walltime": 720000,
        "num_procs_limit": 992,
        "queue_name": "batch_l2"
    },
    "batch_r": {
        "max_walltime": 604800,
        "num_procs_limit": 1024,
        "queue_name": "batch_r"
    },
    "batch_r1": {
        "max_walltime": 604800,
        "num_procs_limit": 944,
        "queue_name": "batch_r1"
    },
    "batch_r2": {
        "max_walltime": 172800,
        "num_procs_limit": 128,
        "queue_name": "batch_r2"
    },
    "debug": {
        "queue_name": "debug"
    },
    "debug_r": {
        "max_walltime": 1800,
        "num_procs_limit": 16,
        "queue_name": "debug_r"
    },
    "debug_r1": {
        "max_walltime": 1800,
        "num_procs_limit": 16,
        "queue_name": "debug_r1"
    }
}
Cluster gordon.sdsc.xsede.org:
{
    "normal": {
        "max_walltime": 1209600,
        "num_procs_limit": 1024,
        "queue_name": "normal"
    },
    "vsmp": {
        "max_walltime": 1209600,
        "num_procs_limit": 16,
        "queue_name": "vsmp"
    }
}
Cluster trestles.sdsc.xsede.org:
{
    "normal": {
        "max_walltime": 172800,
        "num_procs_limit": 1024,
        "queue_name": "normal"
    },
    "shared": {
        "max_walltime": 172800,
        "num_procs_limit": 1024,
        "queue_name": "shared"
    }
}
SUCCEED!
******    Step 04: show each resource cluster's workload    ******
Cluster stampede.tacc.xsede.org:
{
    "development": {
        "alive_nodes": 146,
        "alive_procs": 2336,
        "busy_nodes": 124,
        "busy_procs": 1984,
        "free_nodes": 22,
        "free_procs": 352,
        "num_queueing_jobs": 17,
        "num_running_jobs": 27
    },
    "large": {
        "alive_nodes": 6061,
        "alive_procs": 96976,
        "busy_nodes": 5712,
        "busy_procs": 91392,
        "free_nodes": 349,
        "free_procs": 5584,
        "num_queueing_jobs": 5,
        "num_running_jobs": 0
    },
    "largemem": {
        "alive_nodes": 15,
        "alive_procs": 480,
        "busy_nodes": 1,
        "busy_procs": 32,
        "free_nodes": 14,
        "free_procs": 448,
        "num_queueing_jobs": 18,
        "num_running_jobs": 1
    },
    "normal": {
        "alive_nodes": 6061,
        "alive_procs": 96976,
        "busy_nodes": 5712,
        "busy_procs": 91392,
        "free_nodes": 349,
        "free_procs": 5584,
        "num_queueing_jobs": 2019,
        "num_running_jobs": 76
    },
    "serial": {
        "alive_nodes": 40,
        "alive_procs": 640,
        "busy_nodes": 33,
        "busy_procs": 528,
        "free_nodes": 7,
        "free_procs": 112,
        "num_queueing_jobs": 20,
        "num_running_jobs": 0
    }
}
Cluster blacklight.psc.xsede.org:
{
    "batch_l2": {
        "alive_nodes": 256,
        "alive_procs": 4096,
        "busy_nodes": 2,
        "busy_procs": 32,
        "free_nodes": 254,
        "free_procs": 4064,
        "num_queueing_jobs": 0,
        "num_running_jobs": 1
    },
    "batch_r": {
        "alive_nodes": 256,
        "alive_procs": 4096,
        "busy_nodes": 93,
        "busy_procs": 1488,
        "free_nodes": 163,
        "free_procs": 2608,
        "num_queueing_jobs": 67,
        "num_running_jobs": 28
    },
    "batch_r2": {
        "alive_nodes": 256,
        "alive_procs": 4096,
        "busy_nodes": 2,
        "busy_procs": 32,
        "free_nodes": 254,
        "free_procs": 4064,
        "num_queueing_jobs": 0,
        "num_running_jobs": 2
    },
    "debug_r": {
        "alive_nodes": 256,
        "alive_procs": 4096,
        "busy_nodes": 1,
        "busy_procs": 16,
        "free_nodes": 255,
        "free_procs": 4080,
        "num_queueing_jobs": 1,
        "num_running_jobs": 1
    }
}
Cluster gordon.sdsc.xsede.org:
{
    "normal": {
        "alive_nodes": 877,
        "alive_procs": 14032,
        "busy_nodes": 845,
        "busy_procs": 13520,
        "free_nodes": 32,
        "free_procs": 512,
        "num_queueing_jobs": 1753,
        "num_running_jobs": 337
    },
    "vsmp": {
        "alive_nodes": 2,
        "alive_procs": 512,
        "busy_nodes": 0,
        "busy_procs": 0,
        "free_nodes": 2,
        "free_procs": 512,
        "num_queueing_jobs": 0,
        "num_running_jobs": 0
    }
}
Cluster trestles.sdsc.xsede.org:
{
    "normal": {
        "alive_nodes": 287,
        "alive_procs": 9184,
        "busy_nodes": 195,
        "busy_procs": 6240,
        "free_nodes": 92,
        "free_procs": 2944,
        "num_queueing_jobs": 1,
        "num_running_jobs": 49
    },
    "shared": {
        "alive_nodes": 24,
        "alive_procs": 752,
        "busy_nodes": 22,
        "busy_procs": 688,
        "free_nodes": 2,
        "free_procs": 64,
        "num_queueing_jobs": 0,
        "num_running_jobs": 75
    }
}
SUCCEED!
******    Step 05: show bandwidth to destination cluster    ******
Cluster stampede.tacc.xsede.org:
  30.875 MBytes/Second
Cluster blacklight.psc.xsede.org:
  72.875 MBytes/Second
Cluster gordon.sdsc.xsede.org:
  17.25 MBytes/Second
Cluster trestles.sdsc.xsede.org:
  1.925 MBytes/Second
SUCCEED!
